---
title: "Gated Recurring Unit / LSTM: Some language processing, DNA scanning"
date: 2023-03-29T12:00:00+02:00
draft: true
---
> This page is part of the [Hello Deep Learning](../hello-deep-learning) series of blog posts.

Placeholder page. Will mostly be an homage to the essential [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/).

Includes a demo trained on my blog posts that writes pretty plausible sentences.

For example: "Galileo Problems and Indonesia and Safety Tolar experiments and communications are available to investigate our manufacturers also do not specific and lives on the fact that we are going to be the same generation in a single time.  And that's it.   I can report that the communication provider the market is a previously been in a ton of work to learn about the world where one strand shows the protein expression making a satellite, and then just like winning operations (in Europe) taken everything about the same time.  The reader I have left on a lot of research and will still start with the real thing.  Explain the Internet is not the case. "

The network constructs sentences like these character by character, which is quite impressive. It generates valid markdown links ftoo, for example.

Page will also include a demo of how Gated Recurring Units can spot splice junctions in DNA.

In [the next chapter](../dl-what-does-it-all-mean) you can find some philosophizing about what deep learning all means, analogies to biology and what the future might hold.
